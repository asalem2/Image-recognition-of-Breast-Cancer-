{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VUFkpoBvS6ZP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms,models\n",
        "# nn is the pytorch nueral network model.\n",
        "# optim is the pytroch optimizer. This is optimizes the model.\n",
        "# dataloader is for reading and processing different datasets.\n",
        "# torchvision is the module for image processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 80\n",
        "# The model input size should always be fixed. Input_size sets the image size to 80x80 pixels.\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# This checks if the gpu is available, if not it will use cpu. GPU is faster.\n",
        "# In the runtime tab, I've changed the \"runtime type\" to \"T4 GPU4\". This is faster for training the model."
      ],
      "metadata": {
        "id": "flGBTv3DTEIs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device\n",
        "#If you run this cell and it prints \"cuda\" that means GPU is available.\n",
        "#Otherwise it will print \"CPU\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBLF3-rRWUHK",
        "outputId": "27f0a78f-952a-4076-fb6b-7ccca5fbbeb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.Resize((80,80)),transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "# transforms.Resize changes the size of the image\n",
        "# transforms.ToTensor converts the 80x80 image to Tensor. a tensor is a multidimentional array/matrix\n",
        "# transforms.Normalize. This normalizes the pixels values based on the parameters we provide."
      ],
      "metadata": {
        "id": "AIQOuGnaWaiW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=datasets.ImageFolder(\"/content/drive/MyDrive/Dataset_BUSI_with_GT\",transform=transform)\n",
        "#Here we are loading the image folder from the google drive directory.\n",
        "#We are also passing in the transform parameter, which will run the operations listed in the previous cell."
      ],
      "metadata": {
        "id": "TM_SsTvbX_ZI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Here we are just mounting the google drive where the images are stored."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztct0oO9SSxp",
        "outputId": "60a9f48f-3bcf-41ca-8b8b-6a5249a6bae3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8*len(dataset))\n",
        "#Here we are taking 80% of the dataset for training."
      ],
      "metadata": {
        "id": "nLEhQNTFa1Kn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = len(dataset)-train_size\n",
        "#Here we are assigning the remaining 20% for testing."
      ],
      "metadata": {
        "id": "ymwW5Js4bYUK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,test_dataset=torch.utils.data.random_split(dataset,[train_size,test_size])\n",
        "#Now we are splitting the dataset with train_dataset\n",
        "#We are passing in the size train_size and test_size"
      ],
      "metadata": {
        "id": "gtGspNJWbrSe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=True)\n",
        "#Here we are defining how many images to push to the model in each time (each iteration).\n",
        "#We will push 32 images at each instance, and shuffle them so we don't push the same image into the model.\n",
        "#We can decrease the batch size if we want to use less memory (will take longer). Or increase it if we have memory and want to train fast."
      ],
      "metadata": {
        "id": "EIJqTJrDfNrA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.resnet18(pretrained=True)\n",
        "#Here we will define the model.\n",
        "#This will load the pretrained model\n",
        "#Resnet 18 is a model architecture."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A34LlfaccYBr",
        "outputId": "7d8f9077-5281-41d5-d074-9e3967113636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 42.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)\n",
        "#Here we can print out the model to see what it looks like (resnet)"
      ],
      "metadata": {
        "id": "TQWTt71NZulu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc=nn.Linear(model.fc.in_features,3)\n",
        "#We have 3 classes.\n",
        "#Since our images are grouped in three classes, we will check the probability of each class (normal, benign, and malignant)\n",
        "#This is why it's called a classification model."
      ],
      "metadata": {
        "id": "thpOqticc7JI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(device)\n",
        "#Here we will set it to run the model on the device.\n",
        "#When we use GPU or CPU in Pytorch we have to convert the model.\n",
        "#The device parameter means we are using GPU (cuda)\n",
        "#This can be validated above where we call (device) and see \"cuda\" printed."
      ],
      "metadata": {
        "id": "LxYq_xgSdTHV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "#Loss Function\n",
        "#This loss function calculates the deviation value.\n",
        "#Ths will calculate the deviations of the predicted data from the actual data."
      ],
      "metadata": {
        "id": "XLF9tKvFdlgK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=optim.Adam(model.parameters(),0.001)\n",
        "#Here we will set the learning rate to 0.001 for the optimizer\n",
        "#The optimizer is responsible for adjusting the neuron values, so the model can improve it's accuracy."
      ],
      "metadata": {
        "id": "JMIf0lJNdzFP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are defining the function to train the model.\n",
        "#We will pass in the required parameters which we've created\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            #Here we are converting the image data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #Initialize the optimizer\n",
        "            optimizer.zero_grad()\n",
        "            #Push the model inputs to the model, to get the output\n",
        "            outputs = model(inputs)\n",
        "            # Then we are converting the output value from the model to the actual value.\n",
        "            #The loss function is an important function to solve this problem.\n",
        "            loss = criterion(outputs, labels)\n",
        "            #This is the backward propagation (.backward and .step). After calculating the loss it will update the values of the nuerons.\n",
        "            #This is one of th most important parts of the nueral network.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #This is summing the loss for each run.\n",
        "            running_loss += loss.item()\n",
        "            #This is the predicted output (torch.max)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_acc = correct / total\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # Validation loop\n",
        "        # .eval evaluates how well the model performed on the test data set.\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        #Here we are not updating the model parameters with no_grad.\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "              f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    return train_acc_history, val_acc_history\n",
        "\n",
        "# Train the model\n",
        "# Here we are calling the function.\n",
        "# Here we are setting the number of epochs to 10\n",
        "# Here we are passing in the loss function\n",
        "train_acc_history, val_acc_history = train_model(model, train_loader, test_loader, loss_fn, optimizer, 10)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'multi_class_classification_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjvFUcx2eEgL",
        "outputId": "9221fcdf-84ae-453b-c89e-a544d00c1337"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6758, Train Accuracy: 0.7488, Validation Accuracy: 0.8228\n",
            "Epoch [2/10], Loss: 0.3481, Train Accuracy: 0.8629, Validation Accuracy: 0.8038\n",
            "Epoch [3/10], Loss: 0.2651, Train Accuracy: 0.8954, Validation Accuracy: 0.8006\n",
            "Epoch [4/10], Loss: 0.2668, Train Accuracy: 0.8986, Validation Accuracy: 0.7342\n",
            "Epoch [5/10], Loss: 0.1585, Train Accuracy: 0.9437, Validation Accuracy: 0.8291\n",
            "Epoch [6/10], Loss: 0.2085, Train Accuracy: 0.9319, Validation Accuracy: 0.8544\n",
            "Epoch [7/10], Loss: 0.1648, Train Accuracy: 0.9469, Validation Accuracy: 0.8861\n",
            "Epoch [8/10], Loss: 0.0911, Train Accuracy: 0.9699, Validation Accuracy: 0.8956\n",
            "Epoch [9/10], Loss: 0.0727, Train Accuracy: 0.9754, Validation Accuracy: 0.8924\n",
            "Epoch [10/10], Loss: 0.1125, Train Accuracy: 0.9635, Validation Accuracy: 0.8734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyzing the results.\n",
        "#If our loss score is decreasing with each epoch, that is good and means our model is predicting more accurately.\n",
        "#If our accuracy is increasing that is good and means the model is improving"
      ],
      "metadata": {
        "id": "n3qUqaqaepW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}